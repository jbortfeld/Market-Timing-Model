{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Variable Selection\n",
    "\n",
    "In this notebook we'll be looking at different ways of pruning our model to make it as predictive as possible but also as simple as possible. \n",
    "\n",
    "Arguably, the biggest problem in developing a trading strategy is to find one that works in real-life and not just in a backtest. We talked previously about how all sorts of factors conspire to make our backtested strategy seem more impressive than it turns out to be on real data. Data leakage in terms of using future data is one danger. Can you think of another? \n",
    "\n",
    "How about choosing which variables to test in the first place? For instance we know that in 2008 the housing market crashed and that the health of the housing market was a predictor of the stock market. Therefore we include several variables about the housing market (delinquenices and housing starts) amongst our set of variables to test. But including those housing market variables in the first place relies on us knowing that the housing market would crash and take the stock market along with it. \n",
    "\n",
    "Now that I've hopefully implanted some doubt in your mind, let's move on. In the next section from Hull et al. they will address the problem of variable selection. They've defined a set of 15 variables to potentially use in the model. Now they'll decide which variables to actually include."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "<span class=\"label label-primary\"> The Paper </span>\n",
    "<br><br>\n",
    "\n",
    "<a id='snap_back_1'></a>\n",
    "We consider 15 forecasting variables, which means each time we fit the model we need to estimate 15 forecasting coefficients plus the intercept term for a total of 16 parameters.  We limit the number of estimated parameters through variable selection, which leads to more parsimonious models and generally results in better out-of-sample forecasting properties.  <a href='#supplemental_content_1'>Parsimonious</a> models are also easier to interpret and attribute performance.  It is easier to understand which variables contribute to forecasting results when there are fewer variables to consider.  \n",
    "<br><br>\n",
    "\n",
    "<a id='snap_back_2'></a>\n",
    "<a id='snap_back_3'></a>\n",
    "We estimate the WLS specification which incorporates a <a href='#supplemental_content_2'>bidirectional stepwise procedure</a>.  Variables are chosen based on the <a href='#supplemental_content_3'>Akaike Information Criterion (AIC)</a>.  The bidirectional stepwise selection combines forward selection, which starts with no variables in the model and adds variables that capture the largest improvement, and backward elimination, which starts with all candidate variables and removing the least significant variables.  One feature of the stepwise WLS estimation is that the number of selected variables will change, as predictors come in and out of the selected set.  In comparison, in WLS without variable selection, all of the variables always get non-zero weights, even if they only contribute marginally in a given sample.  Hull and Qiao (2017) use correlation screening as their variable selection technique because using overlapping six-month market returns leads to inflated t-statistics and results in misleading traditional likelihood function calculations and AIC values.   \n",
    "<br><br>\n",
    "\n",
    "We estimate the stepwise WLS at the end of each month.  Starting on 03/31/2003, we use 154 months from 06/01/1990 to 03/31/2003 to estimate our model.  We obtain the model parameters on 03/31/2003, which we hold constant from 04/01/2003 to 04/30/2003.  For every day in this month, we use the updated return predictors, along with the fixed model parameters, to produce one-month equity risk premium forecasts on a daily frequency.  On 04/30/2003, we re-estimate our model using an expanding window, from 06/01/1990 to 04/30/2003, to obtain new parameter values which we use for next month’s equity premium forecasts.  We continue to re-estimate our model monthly and make one-month equity premium forecasts every day until the end of the sample.  We publish a summary of our model output in our Daily Report.  A sample of the Daily Report appears in Appendix I. \n",
    "<br><br>\n",
    "\n",
    "\n",
    "2.3 Variable Selection \n",
    "\n",
    "In Figure 1 we look at the identity of the selected variables.  On the vertical axis is the contribution of each explanatory variable towards the <a href='#supplemental_content_4'>total explained variance</a> (Lindman, Merenda, and Gold, 1980; Chevan and Sutherland, 1991).  The stepwise WLS puts zero weight on marginal variables that do not add substantially to the model.  Of the 15 variables we consider, typically only about five to seven are selected at any given time.  There are significant changes in the number and identify of the selected variables.  In contrast, without variable selection, WLS reduces the weight put on marginal variables that do not contribute to the explanatory power of the model, but does not remove them from the model.   \n",
    "<br><br>\n",
    "\n",
    "Consider variable X which does not add any forecasting power to the model.  If we use WLS with variable selection, the marginal contribution of X would be too small and it will be eliminated from our model.  If we use WLS, we always put a positive weight on X.  For out-of-sample prediction, including X only adds noise to our forecast.  The forecasts coming from WLS with variable selection will likely be more stable compared to WLS without variable selection. \n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "![](variable_selection_1.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "\n",
    "Some variables were selected to be in the model throughout the sample, whereas others only contributed to the explanatory power of the model in a fraction of the sample.  CP was selected in the earlier part of the sample until 2009, and then it was eliminated from the model.  BD was important until 2013, and then it was driven out by other variables.  On the one hand, CRP and DL were useful in the first half of the sample but not in the second half.  On the other hand, NAPM and LOAN were only selected in the second half but not in the first half.  In addition, some variables such as EVUSD were almost never selected, but we did not remove them from the pool of candidate variables. \n",
    "<br><br>\n",
    "\n",
    "Predictor variables entering and exiting our model may be due to variables containing overlapping information.  When one variable is dropped from the model, another variable (or several variables) that may share part of the same information set could come into the model.  For example, in 2004 BD (Baltic Dry Index) was temporarily removed from the model and UR (Change in Unemployment Rate) was added.  It is likely that both variables contained useful information about the macroeconomic environment, so when BD was dropped from the model, another variable that contained similar information was added. \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"label label-success\"> Commentary and Supplemental Content </span>\n",
    "<a id='supplemental_content_1'></a>\n",
    "\n",
    "### Definition: Parsimony\n",
    "\n",
    "Merriam-Webster defines \"parsimony\" as\n",
    "\n",
    "1 a : the quality of being careful with money or resources : thrift the necessity of wartime parsimony\n",
    "b : the quality or state of being stingy The charity was surprised by the parsimony of some larger corporations.\n",
    "2 : economy in the use of means to an end; especially : economy of explanation in conformity with Occam's razor\n",
    "the scientific law of parsimony dictates that any example of animal behavior should be interpreted at its simplest, most immediate level —Peter Gorner\n",
    "\n",
    "<a href='#snap_back_1'>go back to main body</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"label label-success\"> Commentary and Supplemental Content </span>\n",
    "<a id='supplemental_content_2></a>\n",
    "\n",
    "### Stepwise Variable Selection\n",
    "\n",
    "<a href='#snap_back_2'>go back to main body</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"label label-success\"> Commentary and Supplemental Content </span>\n",
    "<a id='supplemental_content_3></a>\n",
    "\n",
    "### Comparing Models with the Akaike Information Criterion (AIC)\n",
    "\n",
    "<a href='#snap_back_3'>go back to main body</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
