{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import UserDefinedFunctions\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels\n",
    "import itertools\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Variable Selection\n",
    "\n",
    "In this notebook we'll be looking at different ways of pruning our model to make it as predictive as possible but also as simple as possible. \n",
    "\n",
    "Arguably, the biggest problem in developing a trading strategy is to find one that works in real-life and not just in a backtest. We talked previously about how all sorts of factors conspire to make our backtested strategy seem more impressive than it turns out to be on real data. Data leakage in terms of using future data is one danger. Can you think of another? \n",
    "\n",
    "How about choosing which variables to test in the first place? For instance we know that in 2008 the housing market crashed and that the health of the housing market was a predictor of the stock market. Therefore we include several variables about the housing market (delinquenices and housing starts) amongst our set of variables to test. But including those housing market variables in the first place relies on us knowing that the housing market would crash and take the stock market along with it. \n",
    "\n",
    "Now that I've hopefully implanted some doubt in your mind, let's move on. In the next section from Hull et al. they will address the problem of variable selection. They've defined a set of 15 variables to potentially use in the model. Now they'll decide which variables to actually include."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "<span class=\"label label-primary\"> The Paper </span>\n",
    "<br><br>\n",
    "\n",
    "<a id='snap_back_1'></a>\n",
    "We consider 15 forecasting variables, which means each time we fit the model we need to estimate 15 forecasting coefficients plus the intercept term for a total of 16 parameters.  We limit the number of estimated parameters through variable selection, which leads to more parsimonious models and generally results in better out-of-sample forecasting properties.  <a href='#supplemental_content_1'>Parsimonious</a> models are also easier to interpret and attribute performance.  It is easier to understand which variables contribute to forecasting results when there are fewer variables to consider.  \n",
    "<br><br>\n",
    "\n",
    "<a id='snap_back_2'></a>\n",
    "<a id='snap_back_3'></a>\n",
    "We estimate the WLS specification which incorporates a <a href='#supplemental_content_2'>bidirectional stepwise procedure</a>.  Variables are chosen based on the <a href='#supplemental_content_3'>Akaike Information Criterion (AIC)</a>.  The bidirectional stepwise selection combines forward selection, which starts with no variables in the model and adds variables that capture the largest improvement, and backward elimination, which starts with all candidate variables and removing the least significant variables.  One feature of the stepwise WLS estimation is that the number of selected variables will change, as predictors come in and out of the selected set.  In comparison, in WLS without variable selection, all of the variables always get non-zero weights, even if they only contribute marginally in a given sample.  Hull and Qiao (2017) use correlation screening as their variable selection technique because using overlapping six-month market returns leads to inflated t-statistics and results in misleading traditional likelihood function calculations and AIC values.   \n",
    "<br><br>\n",
    "\n",
    "We estimate the stepwise WLS at the end of each month.  Starting on 03/31/2003, we use 154 months from 06/01/1990 to 03/31/2003 to estimate our model.  We obtain the model parameters on 03/31/2003, which we hold constant from 04/01/2003 to 04/30/2003.  For every day in this month, we use the updated return predictors, along with the fixed model parameters, to produce one-month equity risk premium forecasts on a daily frequency.  On 04/30/2003, we re-estimate our model using an expanding window, from 06/01/1990 to 04/30/2003, to obtain new parameter values which we use for next monthâ€™s equity premium forecasts.  We continue to re-estimate our model monthly and make one-month equity premium forecasts every day until the end of the sample.  We publish a summary of our model output in our Daily Report.  A sample of the Daily Report appears in Appendix I. \n",
    "<br><br>\n",
    "\n",
    "\n",
    "2.3 Variable Selection \n",
    "\n",
    "\n",
    "<a id='snap_back_4'></a>\n",
    "In Figure 1 we look at the identity of the selected variables.  On the vertical axis is the contribution of each explanatory variable towards the <a href='#supplemental_content_4'>total explained variance</a> (Lindman, Merenda, and Gold, 1980; Chevan and Sutherland, 1991).  The stepwise WLS puts zero weight on marginal variables that do not add substantially to the model.  Of the 15 variables we consider, typically only about five to seven are selected at any given time.  There are significant changes in the number and identify of the selected variables.  In contrast, without variable selection, WLS reduces the weight put on marginal variables that do not contribute to the explanatory power of the model, but does not remove them from the model.   \n",
    "<br><br>\n",
    "\n",
    "Consider variable X which does not add any forecasting power to the model.  If we use WLS with variable selection, the marginal contribution of X would be too small and it will be eliminated from our model.  If we use WLS, we always put a positive weight on X.  For out-of-sample prediction, including X only adds noise to our forecast.  The forecasts coming from WLS with variable selection will likely be more stable compared to WLS without variable selection. \n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "![](variable_selection_1.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "\n",
    "Some variables were selected to be in the model throughout the sample, whereas others only contributed to the explanatory power of the model in a fraction of the sample.  CP was selected in the earlier part of the sample until 2009, and then it was eliminated from the model.  BD was important until 2013, and then it was driven out by other variables.  On the one hand, CRP and DL were useful in the first half of the sample but not in the second half.  On the other hand, NAPM and LOAN were only selected in the second half but not in the first half.  In addition, some variables such as EVUSD were almost never selected, but we did not remove them from the pool of candidate variables. \n",
    "<br><br>\n",
    "\n",
    "Predictor variables entering and exiting our model may be due to variables containing overlapping information.  When one variable is dropped from the model, another variable (or several variables) that may share part of the same information set could come into the model.  For example, in 2004 BD (Baltic Dry Index) was temporarily removed from the model and UR (Change in Unemployment Rate) was added.  It is likely that both variables contained useful information about the macroeconomic environment, so when BD was dropped from the model, another variable that contained similar information was added. \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"label label-warning\"> Assessment: Remember </span>\n",
    "\n",
    "What of the following are True?\n",
    "\n",
    " * The AIC metric should only be used with OLS, not WLS\n",
    " * More complex models are always better because they fit the data better\n",
    " * Linear models are by definition parsimonious\n",
    " * The variable with the highest R^2 in a univariate regression (when you regress the target variable on a single explanatory variable only) will indicate the variable that is the most important when you combine all variables in a multivariate regression (when you regress the target variable on all explanatory variables)\n",
    " * R^2 is the best metric to use in stepwise variable selection\n",
    " * AIC is the best metric to use in stepwise variable selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"label label-warning\"> Assessment: Understand </span>\n",
    "    \n",
    "In variable selection we are picking which variables to use out of a set of all possible variables. This is akin to selecting which model to use out of a set of possible models. \n",
    "\n",
    "Hull et al. use 15 explanatory variables in their variable selection process. How many possible models (explanatory variable combinations) are they choosing from? What is the complexity of the \"model space\" as the number of explanatory variables grow? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[your answer here]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Each variable can either be included in the model or excluded from the model. So that's 2 options for var1, 2 options for var2, etc. \n",
    "\n",
    "This will amount to 2x2x2x2x.... or 2^N where there are N possible variables. \n",
    "\n",
    "Given that Hull et al. use 15 variables, that's 2^15 = 32,768 different models. \n",
    "\n",
    "The model complexity is O(2^N). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"label label-warning\"> Assessment: Understand </span>\n",
    "    \n",
    "In the forward variable selection procedure described, we test adding each variable and then actually add the variable with the best performance gain. We then continue doing this until some stoppin criteria. \n",
    "\n",
    "Hull et al. use 15 explanatory variables in their variable selection process. What is the worst-case scenario for the number of tests we must implement? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[your answer here]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: In the first iteration you have to test 15 variables. In the next stage you have to test 14 variables to potentially add in addition to the 1 previously chosen. In the next stage you have to test 13 variables to potentially add to the 2 previously chosen, etc. \n",
    "\n",
    "So you end up running 15 + 14 + 13 +... + 1 = 120 tests. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"label label-warning\"> Assessment: Understand </span>\n",
    "    \n",
    "    \n",
    "Let's develop some sense of the relationship between model performance and the number of variables we use. Plot R^2 vs the number of explanatory variables. Is performance increasing, decreasing or constant in the number of explanatory variables? You can add variables in any ordering as it doesn't particularly matter for this exercise. \n",
    "\n",
    "Do the same using AIC as your performance measure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2542dfa0198>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAF5CAYAAADDDWPBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2542df9dfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#######################################\n",
    "# SAMPLE\n",
    "# DO NOT MAKE ANY CHANGES IN THIS CELL\n",
    "#######################################\n",
    "\n",
    "# get the input data\n",
    "training_data = pd.read_hdf('training_data.hdf')\n",
    "\n",
    "all_possible_vars = ['industrial_production', 'change_inflation', 'credit_risk_premium', 'slope_interest_rate',\n",
    "         'housing_starts', 'delinquencies', 'change_unemployment']\n",
    "\n",
    "vars_to_use = []\n",
    "\n",
    "# save the performance of each model\n",
    "num_vars = []\n",
    "r_squared_values = []\n",
    "aic_values = []\n",
    "\n",
    "# add each variable to the explanatory variable set, one at a time\n",
    "for var in all_possible_vars:\n",
    "    \n",
    "    # add the variable to the variables you are using\n",
    "    vars_to_use.append(var)\n",
    "\n",
    "    results = UserDefinedFunctions.WLS_regression(training_data, x_vars = vars_to_use,  rho = 0.99)\n",
    "    current_aic = results['aic']\n",
    "    current_r_squared = results['r_squared']\n",
    "    \n",
    "    r_squared_values.append(current_r_squared)\n",
    "    aic_values.append(current_aic)\n",
    "    num_vars.append(len(vars_to_use))\n",
    "    \n",
    "# make a dataframe with the results\n",
    "df = pd.DataFrame({'num_vars': num_vars,\n",
    "                  'r_squared': r_squared_values,\n",
    "                  'aic': aic_values})\n",
    "\n",
    "df.set_index('num_vars')['r_squared'].plot(title = 'R^2 vs. Num Vars')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2542df9c780>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhUAAAGHCAYAAAAHoqCrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxf8D/9bKRtg/Ib0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2542e0db9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.set_index('num_vars')['aic'].plot(title = 'AIC vs. Num Vars')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"label label-warning\"> Assessment: Apply </span>\n",
    "\n",
    "As you might imagine, now we're going to <b>implement</b> Stepwise Variable Selection. The process is available built-in when using other coding languages like R, Stata, and SAS which are all more statistics-centric in comparison to Python. \n",
    "\n",
    "So what about python? Unfortunately, while scikit learn does have some functionality around feature selection (variable selection), it does not appear that they do stepwise variable selection based on the stackoverflow chatter. Furthermore, as of summer 2018, it doesn't seem like statsmodels does stepwise variable selection either. \n",
    "\n",
    "http://scikit-learn.org/stable/modules/feature_selection.html\n",
    "\n",
    "\n",
    "Furthermore, even if they did do stepwise variable selection for regular regression (OLS), we'd want to make sure it's compatible with WLS. Luckily, stepwise variable selection isn't that complicated and the internet provides some resources:\n",
    "    \n",
    "https://planspace.org/20150423-forward_selection_with_statsmodels/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](variable_selection_3.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn. Adapt the code above (or just write your own code from scratch) to perform stepwise variable selection. For simplicity, you can use either forward, backward or bi-directional selection. Use AIC as your performance measure when evaluating different models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1996-12-31 00:00:00\n",
      "2013-07-31 00:00:00\n",
      "starting AIC with all vars:  -581.515804198\n",
      "\n",
      "iteration number:  0\n",
      "--removing industrial_production results in -583.4986200048713\n",
      "--removing change_inflation results in -582.7477195071428\n",
      "--removing credit_risk_premium results in -582.6935113550799\n",
      "--removing slope_interest_rate results in -581.5906036545709\n",
      "--removing housing_starts results in -580.818327495145\n",
      "--removing delinquencies results in -583.2702910941025\n",
      "--removing change_unemployment results in -581.4471194812988\n",
      "done with iteration. Remove industrial_production and new best aic is -583.4986200048713\n",
      "\n",
      "iteration number:  1\n",
      "--removing change_inflation results in -584.743116049038\n",
      "--removing credit_risk_premium results in -584.6182592003686\n",
      "--removing slope_interest_rate results in -583.5799414249491\n",
      "--removing housing_starts results in -582.8057640930858\n",
      "--removing delinquencies results in -585.2519825873921\n",
      "--removing change_unemployment results in -583.4455910257295\n",
      "done with iteration. Remove delinquencies and new best aic is -585.2519825873921\n",
      "\n",
      "iteration number:  2\n",
      "--removing change_inflation results in -586.6862428892186\n",
      "--removing credit_risk_premium results in -585.5340017648532\n",
      "--removing slope_interest_rate results in -585.5361290067126\n",
      "--removing housing_starts results in -584.626700930334\n",
      "--removing change_unemployment results in -585.4110150799161\n",
      "done with iteration. Remove change_inflation and new best aic is -586.6862428892186\n",
      "\n",
      "iteration number:  3\n",
      "--removing credit_risk_premium results in -586.1335362288823\n",
      "--removing slope_interest_rate results in -587.5225567903587\n",
      "--removing housing_starts results in -586.0153548462463\n",
      "--removing change_unemployment results in -586.526052266833\n",
      "done with iteration. Remove slope_interest_rate and new best aic is -587.5225567903587\n",
      "\n",
      "iteration number:  4\n",
      "--removing credit_risk_premium results in -588.0658338981648\n",
      "--removing housing_starts results in -587.0638059420844\n",
      "--removing change_unemployment results in -587.5225176367294\n",
      "done with iteration. Remove credit_risk_premium and new best aic is -588.0658338981648\n",
      "\n",
      "iteration number:  5\n",
      "--removing housing_starts results in -587.5650650899377\n",
      "--removing change_unemployment results in -588.9417857479943\n",
      "done with iteration. Remove change_unemployment and new best aic is -588.9417857479943\n",
      "\n",
      "iteration number:  6\n",
      "--removing housing_starts results in -587.7846392097204\n",
      "break out: 999999999.0\n",
      "done with iteration. Remove change_unemployment and new best aic is -588.9417857479943\n",
      "\n",
      "final variables to use: ['housing_starts']\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "# SAMPLE\n",
    "# DO NOT MAKE ANY CHANGES IN THIS CELL\n",
    "#######################################\n",
    "\n",
    "\n",
    "# BACKWARD STEPWISE VARIABLE SELECTION WITH ACI\n",
    "\n",
    "\n",
    "# get the input data\n",
    "training_data = pd.read_hdf('training_data.hdf')\n",
    "\n",
    "# start with all variables \n",
    "all_possible_vars = ['industrial_production', 'change_inflation', 'credit_risk_premium', 'slope_interest_rate',\n",
    "         'housing_starts', 'delinquencies', 'change_unemployment']\n",
    "\n",
    "# define the dataset you want to run the regression on\n",
    "temp = training_data.iloc[0:200]\n",
    "print(temp['portfolio_date'].min())\n",
    "print(temp['portfolio_date'].max())\n",
    "\n",
    "# get the performance (AIC) with all variables included in the model\n",
    "results = UserDefinedFunctions.WLS_regression(temp, x_vars = all_possible_vars,  rho = 0.99)\n",
    "current_aic = results['aic']\n",
    "print('starting AIC with all vars: ', current_aic)\n",
    "print()\n",
    "\n",
    "# define the variables we want to use\n",
    "# (again, starting with all variables)\n",
    "vars_to_use = all_possible_vars.copy()\n",
    "\n",
    "search = True\n",
    "\n",
    "iterations = 0\n",
    "while search:\n",
    "\n",
    "    print('iteration number: ', iterations)\n",
    "    iterations += 1\n",
    "    \n",
    "    best_candidate_aic = 999999999.\n",
    "    \n",
    "    # iterate through variables\n",
    "    for var in all_possible_vars:    \n",
    "\n",
    "        # if variable is in the list of variables we are using, then test removing it\n",
    "        if var in vars_to_use:\n",
    "\n",
    "            vars_to_try = vars_to_use.copy()\n",
    "\n",
    "            # remove the candidate variable \n",
    "            vars_to_try.remove(var)\n",
    "\n",
    "            # estimate model\n",
    "            results = UserDefinedFunctions.WLS_regression(temp, \n",
    "                                                          x_vars = vars_to_try, \n",
    "                                                          rho = 0.99)\n",
    "            # get the performance of this model\n",
    "            performance = results['aic']\n",
    "            print('--removing {} results in {}'.format(var, performance))\n",
    "\n",
    "            # if removing this variable improves the aic, then consider adding it\n",
    "            if performance < current_aic:\n",
    "\n",
    "                if performance < best_candidate_aic:\n",
    "\n",
    "                    candidate_variable = var\n",
    "                    best_candidate_aic = performance\n",
    "    \n",
    "    # if removing a variable doesn't improve performance, then strop\n",
    "    if best_candidate_aic > current_aic:\n",
    "        search = False\n",
    "        print('break out: {}'.format(best_candidate_aic))\n",
    "        \n",
    "    # if removing a variable improves performance, then remove it and keep testing\n",
    "    else:\n",
    "        vars_to_use.remove(candidate_variable)\n",
    "        current_aic = best_candidate_aic\n",
    "    print('done with iteration. Remove {} and new best aic is {}'.format(candidate_variable, current_aic))\n",
    "    print()\n",
    "\n",
    "    \n",
    "print('final variables to use:', vars_to_use) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"label label-warning\"> Assessment: Apply </span>\n",
    "\n",
    "Now <b>implement</b> stepwise variable selection again but change your methodology. So choose a different version of stepwise variable selection (forward, backward, bi-direcitonal) or a different performance measure (R^2, adjusted R^2, RMSE, or anything else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1996-12-31 00:00:00\n",
      "2013-07-31 00:00:00\n",
      "iteration number:  0\n",
      "--adding industrial_production results in -0.005394417627417436\n",
      "--adding change_inflation results in -0.0026016684990153305\n",
      "--adding credit_risk_premium results in -0.0030009344995247655\n",
      "--adding slope_interest_rate results in -0.004841771842645892\n",
      "--adding housing_starts results in 0.01167383684720602\n",
      "--adding delinquencies results in -0.005479681199806574\n",
      "--adding change_unemployment results in 0.004210557998732711\n",
      "done with iteration. Add housing_starts and new best adjusted R^2 is -677.8971790317116\n",
      "\n",
      "iteration number:  1\n",
      "--adding industrial_production results in 0.006305079810794645\n",
      "--adding change_inflation results in 0.008595656299537624\n",
      "--adding credit_risk_premium results in 0.009331913881074683\n",
      "--adding slope_interest_rate results in 0.007124363420789259\n",
      "--adding delinquencies results in 0.0065503714829940884\n",
      "--adding change_unemployment results in 0.012268787435777195\n",
      "done with iteration. Add change_unemployment and new best adjusted R^2 is -677.8971790317116\n",
      "\n",
      "iteration number:  2\n",
      "--adding industrial_production results in 0.0072746147910809356\n",
      "--adding change_inflation results in 0.00972968840708921\n",
      "--adding credit_risk_premium results in 0.014625847461256503\n",
      "--adding slope_interest_rate results in 0.007118128606767082\n",
      "--adding delinquencies results in 0.009565992250557542\n",
      "done with iteration. Add credit_risk_premium and new best adjusted R^2 is -677.8971790317116\n",
      "\n",
      "iteration number:  3\n",
      "--adding industrial_production results in 0.009140519140550474\n",
      "--adding change_inflation results in 0.009163525843355003\n",
      "--adding slope_interest_rate results in 0.015371183158160773\n",
      "--adding delinquencies results in 0.009256280275164674\n",
      "done with iteration. Add slope_interest_rate and new best adjusted R^2 is -677.8971790317116\n",
      "\n",
      "iteration number:  4\n",
      "--adding industrial_production results in 0.009839213304710936\n",
      "--adding change_inflation results in 0.012864733256197747\n",
      "--adding delinquencies results in 0.010115994718026156\n",
      "break out: -999.0\n",
      "done with iteration. Add slope_interest_rate and new best adjusted R^2 is -677.8971790317116\n",
      "\n",
      "final variables to use: ['housing_starts', 'change_unemployment', 'credit_risk_premium', 'slope_interest_rate']\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "# SAMPLE\n",
    "# DO NOT MAKE ANY CHANGES IN THIS CELL\n",
    "#######################################\n",
    "\n",
    "\n",
    "# FORWARD STEPWISE VARIABLE SELECTION with ADJUSTED R^2\n",
    "# (adjusted R^2 is another way of measuring how well a model fits the data while also penalizing\n",
    "# the number of explanatory variables used in the model - google it!)\n",
    "\n",
    "# get the input data\n",
    "training_data = pd.read_hdf('training_data.hdf')\n",
    "\n",
    "all_possible_vars = ['industrial_production', 'change_inflation', 'credit_risk_premium', 'slope_interest_rate',\n",
    "         'housing_starts', 'delinquencies', 'change_unemployment']\n",
    "\n",
    "# recall that high R^2 is good in contrast to low AIC being good\n",
    "# so set the initial R^2 performance really low\n",
    "current_performance = -999\n",
    "\n",
    "# get some training data\n",
    "temp = training_data.iloc[0:200]\n",
    "print(temp['portfolio_date'].min())\n",
    "print(temp['portfolio_date'].max())\n",
    "\n",
    "# define the variables we want to use\n",
    "# initialize as empty list \n",
    "# (start with no variables since this is using forward variable selection)\n",
    "vars_to_use = []\n",
    "\n",
    "search = True\n",
    "\n",
    "iterations = 0\n",
    "while search:\n",
    "\n",
    "    print('iteration number: ', iterations)\n",
    "    iterations += 1\n",
    "    \n",
    "    best_candidate_performance = -999.\n",
    "    \n",
    "    # iterate through variables\n",
    "    for var in all_possible_vars:    \n",
    "\n",
    "        # if variable is not already in the list of variables we are using, then test adding it\n",
    "        if not var in vars_to_use:\n",
    "\n",
    "            vars_to_try = vars_to_use.copy()\n",
    "\n",
    "            # add the candidate variable to the list of variables already in use\n",
    "            vars_to_try.append(var)\n",
    "\n",
    "            # estimate model\n",
    "            results = UserDefinedFunctions.WLS_regression(temp, \n",
    "                                                          x_vars = vars_to_try, \n",
    "                                                          rho = 0.99)\n",
    "            \n",
    "            # get the performance of this model\n",
    "            # save as a tuple (performance, list of vars)\n",
    "            performance = results['r_squared_adjusted']\n",
    "            print('--adding {} results in {}'.format(var, performance))\n",
    "\n",
    "            # if adding this variable improves the r^2, then consider adding it\n",
    "            if performance > current_performance:\n",
    "\n",
    "                if performance > best_candidate_performance:\n",
    "\n",
    "                    candidate_variable = var\n",
    "                    best_candidate_performance = performance\n",
    "    \n",
    "    # if adding any variable doesn't increase model performance, then stop\n",
    "    if best_candidate_performance < current_performance:\n",
    "        search = False\n",
    "        print('break out: {}'.format(best_candidate_performance))\n",
    "        \n",
    "    # if adding a variable increases model performance, then do it\n",
    "    else:\n",
    "        vars_to_use.append(candidate_variable)\n",
    "        current_performance = best_candidate_performance\n",
    "    print('done with iteration. Add {} and new best adjusted R^2 is {}'.format(candidate_variable, best_candidate_performance))\n",
    "    print()\n",
    "\n",
    "    \n",
    "print('final variables to use:', vars_to_use) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"label label-warning\"> Assessment: Analyze </span>\n",
    "\n",
    "So now you have implemented two methods for variable selection. Run both of them on the same set of data and <b>compare/constrast</b> the results. Do they produce the same set of variables? If not, what is the difference in performance between the two models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[your answer here]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variables selected using stepwise forward selection with adjusted R^2:\n",
      "['change_unemployment', 'slope_interest_rate', 'const', 'housing_starts', 'credit_risk_premium']\n",
      "R^2: 0.03701137693490453\n",
      "adjusted R^2: 0.015371183158160773\n",
      "AIC: -586.6862428892186\n",
      "MSE (mean squared error): 0.0009304005365297672\n",
      "log likelihood: 298.3431214446093\n",
      "\n",
      "variables selected using stepwise backward selection with AIC:\n",
      "['const', 'housing_starts']\n",
      "R^2: 0.01710420038101268\n",
      "adjusted R^2: 0.01167383684720602\n",
      "AIC: -588.9417857479943\n",
      "MSE (mean squared error): 0.0009304005365297672\n",
      "log likelihood: 296.4708928739972\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "# SAMPLE\n",
    "# DO NOT MAKE ANY CHANGES IN THIS CELL\n",
    "#######################################\n",
    "\n",
    "\n",
    "\n",
    "# let's define the set of variables to test\n",
    "all_possible_vars = ['industrial_production', 'change_inflation', 'credit_risk_premium', 'slope_interest_rate',\n",
    "                     'housing_starts', 'delinquencies', 'change_unemployment']\n",
    "\n",
    "# let's define a training data set\n",
    "# (arbitrarily just take the data through JUly 2013)\n",
    "temp = training_data[training_data['portfolio_date'] <= pd.to_datetime('2013-07-31')]\n",
    "\n",
    "# analyze the data using forward variable selection based on adjusted R^2\n",
    "results1 = UserDefinedFunctions.WLS_regression_with_var_selection_r2(temp,\n",
    "                                                         all_possible_vars = all_possible_vars,\n",
    "                                                          rho = 0.99,\n",
    "                                                          verbose = False)\n",
    "print('variables selected using stepwise forward selection with adjusted R^2:')\n",
    "print(results1['model_vars'])\n",
    "print('R^2: {}'.format(results1['r_squared']))\n",
    "print('adjusted R^2: {}'.format(results1['r_squared_adjusted']))\n",
    "print('AIC: {}'.format(results1['aic']))\n",
    "print('MSE (mean squared error): {}'.format(results1['mse']))\n",
    "print('log likelihood: {}'.format(results1['llf']))\n",
    "print()\n",
    "\n",
    "# analyze the data using backwards variable selection based AIC\n",
    "results2 = UserDefinedFunctions.WLS_regression_with_var_selection_aic(temp,\n",
    "                                                         all_possible_vars = all_possible_vars,\n",
    "                                                          rho = 0.99,\n",
    "                                                          verbose = False)\n",
    "print('variables selected using stepwise backward selection with AIC:')\n",
    "print(results2['model_vars'])\n",
    "print('R^2: {}'.format(results2['r_squared']))\n",
    "print('adjusted R^2: {}'.format(results2['r_squared_adjusted']))\n",
    "print('AIC: {}'.format(results2['aic']))\n",
    "print('MSE (mean squared error): {}'.format(results2['mse']))\n",
    "print('log likelihood: {}'.format(results2['llf']))\n",
    "print()\n",
    "\n",
    "# the two variable selection approaches select different explanatory variables. It is difficult to compare performance but the \n",
    "# model based on forward variable selection with adjusted R^2 seems to do better. It's unclear just HOW much better it does and \n",
    "# whether the difference is statistically or economically significant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"label label-warning\"> Assessment: Evaluate </span>\n",
    "\n",
    "Based on your results above, critique the two different variable selection methods you chose. Then pick one and justify why you want to use it going forward. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[you answer here]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example code we will use forward variable selection with adjusted R^2. The model it selects has higher performance than the model from backward variable selection with AIC and it has higher performance when measured across multiple metrics. Furthermore, from a \"marketing\" perspective it's easier to sell a model with four variables rather than just one. While from a scientific/analytical perspective we may prefer parsimony, it'll be difficult to convince clients that you have a model that they should pay for if the model is too simple. \n",
    "\n",
    "Note however, that my comparison on forward selection with adjusted R^2 vs. backwards selection with AIC is based on an analysis of running the two approaches on only one date. In this experiment we know we would want to estimate the model anew every single month. For simplicity, and as an illustration only, we only did the comparison for a single date. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"label label-warning\"> Assessment: Understand</span>\n",
    "    \n",
    "In the exercises above you selected a smaller set of variables to use from the larger set of all possible variables (or maybe you ended up selecting all the variables?). Now we want to start thinking about ways of measuring the relative importance of each variable. Hull et al. reference using the total explained variance. Under this framework, if we want to measure the importance of some variable var1, we would consider adding var1 to all other variable combinations of the other variables and then measure the average performance gain (via R^2) across each time we add var1. \n",
    "\n",
    "So for example, what would the performance gain be if we added var1 to a model that was originally based on just var3 and var4? And what would the performance gain be if we added var1 to a model that was originally based on var3, var5, and var6? Now do that for all possible models and take the average performance gain. \n",
    "\n",
    "When we're done testing adding var1 we move on to testing the addition of var2, etc. \n",
    "\n",
    "Finally, we can rank the importance of each variable by just summing all the average performance gains for each variable and then take the proportion of each variables contribution to that total. \n",
    "\n",
    "Did you get that? So if we have 3 total possible variables here are the possible models: \n",
    "\n",
    "[var1]\n",
    "\n",
    "[var2]\n",
    "\n",
    "[var3]\n",
    "\n",
    "[var1, var2]\n",
    "\n",
    "[var1, var3]\n",
    "\n",
    "[var2, var3]\n",
    "\n",
    "[var1, var2,  var3]\n",
    "\n",
    "What is the number of possible models given N total possible variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[your answer here]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:\n",
    "\n",
    "Given N total variables, we could have a model with just 1 variable, or a model with just 2 variables, ... or a model with all N variables. \n",
    "\n",
    "How many 1-variable models are there? There are N of them.\n",
    "\n",
    "How many 2-variable models are there? If N were 7 then there are \"7 choose 2\" possible unique combinations. Turning to google and just entering \"7 choose 2\" shows yields 21. \n",
    "\n",
    "So ultimately there are (N choose 1) + (N choose 2) + ... (N choose N)\n",
    "\n",
    "If N is equal to 7, then this would result in 127 total possible models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"label label-warning\"> Assessment: Apply</span>\n",
    "\n",
    "Now implement code to measure relative variable importance across a set of selected variables. So if you end up selecting five variables for the model in July 2013, you'll want to assess the relative importance of just those five variables relative to one another. \n",
    "\n",
    "Here's some helper tips on generating possible variable combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('var0', 'var1', 'var2')\n",
      "('var0', 'var1', 'var3')\n",
      "('var0', 'var1', 'var4')\n",
      "('var0', 'var2', 'var3')\n",
      "('var0', 'var2', 'var4')\n",
      "('var0', 'var3', 'var4')\n",
      "('var1', 'var2', 'var3')\n",
      "('var1', 'var2', 'var4')\n",
      "('var1', 'var3', 'var4')\n",
      "('var2', 'var3', 'var4')\n"
     ]
    }
   ],
   "source": [
    "#######################################\n",
    "# SAMPLE\n",
    "# DO NOT MAKE ANY CHANGES IN THIS CELL\n",
    "#######################################\n",
    "\n",
    "\n",
    "\n",
    "all_vars = ['var0', 'var1', 'var2', 'var3', 'var4' ]\n",
    "\n",
    "# generate all possible combinations of 3 variables from the list above\n",
    "combinations = itertools.combinations(all_vars, 3)\n",
    "\n",
    "# print the results\n",
    "for c in combinations:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "industrial_production average gain: 0.012101171380604386\n",
      "change_inflation average gain: 0.012716932091700859\n",
      "credit_risk_premium average gain: 0.01137665330247495\n",
      "slope_interest_rate average gain: 0.011845695427337228\n",
      "housing_starts average gain: 0.012512316127623902\n",
      "delinquencies average gain: 0.012293318207050031\n",
      "change_unemployment average gain: 0.00247940418825438\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>index</th>\n",
       "      <th>change_inflation</th>\n",
       "      <th>change_unemployment</th>\n",
       "      <th>credit_risk_premium</th>\n",
       "      <th>delinquencies</th>\n",
       "      <th>housing_starts</th>\n",
       "      <th>industrial_production</th>\n",
       "      <th>slope_interest_rate</th>\n",
       "      <th>portfolio_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.168826</td>\n",
       "      <td>0.032916</td>\n",
       "      <td>0.151033</td>\n",
       "      <td>0.163203</td>\n",
       "      <td>0.16611</td>\n",
       "      <td>0.160652</td>\n",
       "      <td>0.15726</td>\n",
       "      <td>2018-06-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "index  change_inflation  change_unemployment  credit_risk_premium  \\\n",
       "0              0.168826             0.032916             0.151033   \n",
       "\n",
       "index  delinquencies  housing_starts  industrial_production  \\\n",
       "0           0.163203         0.16611               0.160652   \n",
       "\n",
       "index  slope_interest_rate portfolio_date  \n",
       "0                  0.15726     2018-06-30  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######################################\n",
    "# SAMPLE\n",
    "# DO NOT MAKE ANY CHANGES IN THIS CELL\n",
    "#######################################\n",
    "\n",
    "\n",
    "\n",
    "data = training_data.copy()\n",
    "rho = 0.99\n",
    "\n",
    "all_possible_vars = ['industrial_production', 'change_inflation', 'credit_risk_premium', 'slope_interest_rate',\n",
    "         'housing_starts', 'delinquencies', 'change_unemployment']\n",
    "\n",
    "# initialize a dictionary to hold the performance gains for each variable\n",
    "performance_gains = {var: [] for var in all_possible_vars}\n",
    "\n",
    "# try adding 1 variable to the 'prior model'\n",
    "for i in range(0, len(all_possible_vars) + 1):\n",
    "    \n",
    "    # get all combination of i variables from the set of N possible variables\n",
    "    # this is the \"prior model\"\n",
    "    combinations = itertools.combinations(all_possible_vars, i)\n",
    "    \n",
    "    # iterate through every \"prior model\"\n",
    "    # ie the model you want to compare against when you add in your selected variable\n",
    "    for c in combinations:\n",
    "        \n",
    "        # get the performance (R^2) if you use the specified combination of variables in the \"prior model\"\n",
    "        if c == ():\n",
    "            prior_r_squared = 0.0\n",
    "        else:\n",
    "            prior_r_squared = UserDefinedFunctions.WLS_regression(data, \n",
    "                   x_vars = list(c),\n",
    "                   rho = rho)['r_squared']\n",
    "        \n",
    "        # try adding a variable to the combination above\n",
    "        for var in all_possible_vars:\n",
    "            \n",
    "            # first, check if the variable is already included in the \"prior model\"\n",
    "            if not var in c:\n",
    "                \n",
    "                # add the 'new variable' to the 'prior model' \n",
    "                vars_to_test = list(c)\n",
    "                vars_to_test.append(var)\n",
    "                \n",
    "                # get the performance of the 'new model'\n",
    "                new_r_squared = UserDefinedFunctions.WLS_regression(data, \n",
    "                   x_vars = vars_to_test,\n",
    "                   rho = rho)['r_squared']\n",
    "                \n",
    "                # calculate the gain between new and old models\n",
    "                gain = new_r_squared = prior_r_squared\n",
    "                \n",
    "                performance_gains[var].append(gain)\n",
    "\n",
    "# see the average performance gain for each variable\n",
    "for var in all_possible_vars:\n",
    "    print('{} average gain: {}'.format(var, np.mean(performance_gains[var])))\n",
    "                \n",
    "# calculate the proportion of total performance gain attributable to each variable\n",
    "df = pd.DataFrame(performance_gains)\n",
    "df = df.mean().to_frame('average_gain')\n",
    "df['total_gain'] = df['average_gain'].sum()\n",
    "df['importance'] = df['average_gain'] / df['total_gain']\n",
    "df.sort_values(by = 'importance', inplace = True)\n",
    "\n",
    "# format data \n",
    "df = df[['importance']]\n",
    "df.reset_index(inplace = True)\n",
    "df = df.pivot_table(values=\"importance\", columns='index')\n",
    "df['portfolio_date'] = data['portfolio_date'].max()\n",
    "df.reset_index(inplace = True, drop = True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"label label-warning\"> Assessment: Analyze </span>\n",
    "    \n",
    "Use the code you developed and analyze the relative weights of a set of explanatory variables for some dataset. Which variables were most important and which were least?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[your answer here]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Answer: Based on the example above, the change in inflation was the most important variable, followed by housing starts and industrial production. Most variables generally had the same level of importance (15-16%) except for the change in unemployment which was by far the least important at only 3%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"label label-warning\"> Assessment: Evaluate </span>\n",
    "\n",
    "Based on your analysis above, assess whether your results make good intuitive sense? Can you construct a narrative to describe the relative weights you are seeing? Do your results give you more or less confidence in the procedure for measuring variable importance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[your answer here]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Answer: I am a bit puzzled that change in unemployment is such a weak predictor of forward market returns. The dataset I used runs from the mid 90s through June 2018 and over that period I would have thought it'd do better. We do use WLS though which means we give greater weight to the recent period. During the last few years, markets have been generally going up though there was significant market turmoil in the first half of 2018 even though the job market has been consistently performing well. That may explain the low weight on the unemployment rate. \n",
    "\n",
    "Overall I would have expected more variation in the importance of variables but all variables seem equally important (except for the unemployment rate). This gives me less confidence in the methodology for measuring variable importance and I'd be interested in alternative measures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"label label-warning\"> Assessment: Create </span>\n",
    "\n",
    "Procedures for variable selection and measuring variable importance are subjective and just because Hull et al. use certain methodologies does not mean that they're the best. Can you think of any other ways you would choose which variables you'd want to use? THis question isn't about searching google but rather it asks you to try to create your own method. Or other ways to measure variable importance?  \n",
    "\n",
    "Discuss your ideas below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[your answer here]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Answer: For variable selection I could see using a decision tree to try to identify which variables are most important (which are chosen by the tree) and then passing only those variables into the linear regression procedure. In this way we combine two different model types. Decision trees are generally recognized to be very good for fature selection because they prioritize using the variables tha generate the most explanatory power at each tree node. \n",
    "\n",
    "To measure variable importance we can again use decision trees where there exists a procedure to measure variable importance by measuring the information gain at each node. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Lesson\n",
    "\n",
    "That's it for this notebook. You can proceed to the next workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[intentionally left blank]\n",
    "<br><br>\n",
    "<br><br>\n",
    "<br><br>\n",
    "<br><br>\n",
    "<br><br>\n",
    "<br><br>\n",
    "<br><br>\n",
    "<br><br>\n",
    "<br><br>\n",
    "<br><br>\n",
    "<br><br>\n",
    "<br><br>\n",
    "<br><br>\n",
    "<br><br>\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"label label-success\"> Commentary and Supplemental Content </span>\n",
    "<a id='supplemental_content_1'></a>\n",
    "\n",
    "### Definition: Parsimony\n",
    "\n",
    "Merriam-Webster defines \"parsimony\" as\n",
    "\n",
    "1. the quality of being careful with money or resources : \"thrift the necessity of wartime parsimony\"\n",
    "2. the quality or state of being stingy: \"The charity was surprised by the parsimony of some larger corporations.\"\n",
    "3.  economy in the use of means to an end; especially : economy of explanation in conformity with Occam's razor the scientific law of parsimony dictates that any example of animal behavior should be interpreted at its simplest, most immediate level \n",
    "\n",
    "The third one above and an \"economy of explanation\" seems to fit our context. So this goes back to what we originally said about wanting a model that is as simple as possible. Simplicity (economy) can take many forms including the type of model (simple linear regression versus a recurrent neural network), estimation frequency (coefficient re-estimation frequently versus only once), and number of variables (a few versus a billion).\n",
    "\n",
    "Ok, so by Occam's razor we should prefer simplicity which can be interpreted as prefering fewer explanatory variables to more explanatory variables. Occam aside, why should we prefer that? Ingo Ruczinski, some guy with on the internet, says there are lots of reasons to prefer simplicity:\n",
    "\n",
    " * We want to explain the data in the simplest way â€” redundant predictors should be removed. The principle of Occamâ€™s Razor states that among several plausible explanations for a phenomenon, the simplest is best. Applied to regression analysis, this implies that the smallest model that fits the data is best.\n",
    " * Unnecessary predictors will add noise to the estimation of other quantities that we are interested in. Degrees of freedom will be wasted.\n",
    " * Collinearity is caused by having too many variables trying to do the same job.\n",
    " * Cost: if the model is to be used for prediction, we can save time and/or money by not measuring redundant predictors.\n",
    "\n",
    "http://www.biostat.jhsph.edu/~iruczins/teaching/jf/ch10.pdf\n",
    "\n",
    "Note: Collinearity is a statistical term for having many variables that are all versions of closely related things (ex: industrial production, new car production, ship building, tractor manufacturing, etc). From a statistical perspective, collinearity can make doing statistics difficult (such as in muddying the measured statsitical significance of your explanatory variables).\n",
    "\n",
    "To the list above I'd also add that too many variables increases the risk of overfitting. Given enough explanatory variables we could almost perfectly fit any dataset. For instance, just by random chance we might find that a lot of variables have been correlated to historical stock market returns. So if we include enough random variables, we'll likely include one of those variables that \"accidentally\" predicted the market in the past but there is no rationale to believe it'll predict the market in the future. To be safe (and parsimonious) we should include as few variables in our model as possible to avoid the chance of introducing spurious correlations (but if you like spurious correlations you can find a few here http://www.tylervigen.com/spurious-correlations)\n",
    "\n",
    "<a href='#snap_back_1'>go back to main body</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"label label-success\"> Commentary and Supplemental Content </span>\n",
    "<a id='supplemental_content_2'></a>\n",
    "\n",
    "### Stepwise Variable Selection\n",
    "\n",
    "Back in Notebook 1 we spoke a little about Stepwise Variable Selection. Let's expand on that here.\n",
    "\n",
    "The essence of Stepwise Variable Selection is that we want to only add variables to a model if their contribution in beneficial.  Let's say you have 10 variables you can possibly put into a linear regression model. You start by only including var1 and record how good of a model it is. Then you try var2, then var, etc. After trying them all, you pick the variable that had the best performance and you add that to your model. \n",
    "\n",
    "Then you iterate through the remaining nine variables and find which one, in combination with the variable you added in the previous round, produces the best model. Keep going until you hit some stopping criteria that you define (eg the performance only increases by an arbitrarily small amount).\n",
    "\n",
    "You can do forward variable selection whereby you start with zero variables and add variables one at a time. Or backwards variable selection whereby you start with all variables and subtract variables one at a time. \n",
    "\n",
    "In bi-directional stepwise variable selection we combine the forward and backward procedures. Hull et al. aren't clear on how they implement bi-directional variable selection and the internet provides many variations on the concept. Here's one where you start with all variables intitially included in the model:\n",
    "\n",
    "The process is one of alternation between choosing the least significant variable to drop and then re-considering all dropped variables (except the most recently dropped) for re-introduction into the model. This means that two separate significance levels must be chosen for deletion from the model and for adding to the model. The second significance must be more stringent than the first.\"\n",
    "\n",
    "https://www.stat.ubc.ca/~rollin/teach/643w04/lec/node43.html\n",
    "\n",
    "So in this process, at each iteration we try to drop a variable and then try to add back in a variable (where the performance gain for adding or dropping is satisfied). Why would we ever add back a variable that was previously dropped? Well maybe it's because we don't want to include var1 (say industrial production) when we also are including var6 (unemployment rate) in our model. But as we progress through the stepwise variable selection process, we might find that we end up dropping the unemployment rate from our model which then \"makes room\" for including industrial production. \n",
    "\n",
    "We've been a bit vague in the above discussion by what we mean in terms of model performance. How you choose to measure that is really up to you. Maybe it's statistical significance, or gain in R^2, or drop in RMSE. For Hull et al., they choose to use the Akaike Information Criteria (AIC). \n",
    "\n",
    "For more information on stepwise variable selection, see the collective intelligence of the internet at:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Stepwise_regression\n",
    "\n",
    "\n",
    "<a href='#snap_back_2'>go back to main body</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"label label-success\"> Commentary and Supplemental Content </span>\n",
    "<a id='supplemental_content_3'></a>\n",
    "\n",
    "### Comparing Models with the Akaike Information Criterion (AIC)\n",
    "\n",
    "The AIC is a measure used to compare the performance of different models (like R^2 or RMSE) that specifically incorporates information on the number of explanatory variables you use. The goal should be to minimize the calculated AIC value where a better model fit lowers the metric but more explanatory variables increase the metric. For us parsimony-preferring people, the AIC is a good metric to use and helps us pick between competing models (should we use a model that fits very well but includes 50 variables or a model that fits ok and only includes 5 variables).\n",
    "\n",
    "The AIC metric itself is defined as:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](variable_selection_2.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where L is the likelihood function of the model and K is the number of explanatory variables in your model. \n",
    "\n",
    "L is the measure of how well the model fits the data. You want L to be as big as possible. Alternatively, because you want to minimize AIC, you want K to be as small as possible. \n",
    "\n",
    "Most statisical packages will calculate AIC as part of any linear regression, including the statsmodels library that we've been using for past coding examples. \n",
    "\n",
    "For more information on AIC:\n",
    "\n",
    "https://www.youtube.com/watch?v=7XAHjm6Vy5k\n",
    "\n",
    "<a href='#snap_back_3'>go back to main body</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"label label-success\"> Commentary and Supplemental Content </span>\n",
    "<a id='supplemental_content_4'></a>\n",
    "\n",
    "### Which Variables are the Most Important\n",
    "\n",
    "Let's imagine that you have a model that does a good job of predicting the S&P 500 based on 15 explanatory variables (say the ones selected by Hell et al.). You go to an investor and offer to manage their money using your strategy. You say \"Look at all the money I can make you based on the historical performance.\" The investor is undoubtedly impressed but will inevitably want to know more about what's underneath the hood of your strategy before giving you money and therefore will ask \"which variables are most important when you make your prediction?\"\n",
    "\n",
    "This is a bit of a hard question and there are several \"reasonable\" ways of approaching a solution. \n",
    "\n",
    "Hull et al. reference the method by Lindeman, Merenda, and Gold. Here's a description of an implementation of their method written for R: \n",
    "\n",
    "\"R^2 represents the proportion of variance explained by a set of predictors. If one can estimate the proportion of the R^2 contributed by each individual predictor, the one with larger R^2 would be more important to explain the outcome variable. However, the difficulty lies in how to get the R^2 for each predictor.\n",
    "\n",
    "The most intuitive way to decompose the total R^2 is to add the predictors to the regression model sequentially. Then, the increased R^2 can be considered as the contribution by the predictor just added. However, this method depends on the sequence the predictors are added if the predictors are correlated. \n",
    "\n",
    "The lmg approach is based on sequential R^2 but takes care of the dependence on orderings by averaging over orderings. For example, for a model with 4 predictors, there are a total of 24 orderings. For each ordering, the contributed R^2 can be calculated. lmg is the average of the R^2 across the 24 orderings.\"\n",
    "\n",
    "https://advstats.psychstat.org/book/mregression/importance.php\n",
    "\n",
    "So in other words: to measure a variable's importance we're going to measure the additional R^2 gained whenever we add that variable to the model. And we're going to add the variable to all possible models (all possible combinations of the variables) and then take the average performance gain.  \n",
    "\n",
    "That sounds reasonable but there are some downsides to this approach. Can you think of any? \n",
    "\n",
    "For another take on variable importance using Lindeman, Merenda, and Gold see here:\n",
    "\n",
    "https://www.r-bloggers.com/the-relative-importance-of-predictors-let-the-games-begin/\n",
    "\n",
    "<a href='#snap_back_4'>go back to main body</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
